# -*- coding: utf-8 -*-
"""PDFSummarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rsargD-som3YV-TtKwyP0Ah56TApAh8l
"""

pip install langchain chromadb pypdf gradio

import gradio as gr
import pdfplumber
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import CTransformers
from langchain.chains import RetrievalQA

# Optimized PDF loading (first 5 pages)
def load_pdf(pdf_path, num_pages=5):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for i, page in enumerate(pdf.pages[:num_pages]):
            text += page.extract_text() + "\n"
    return text

# Optimized summarization
def summarize_pdf(pdf_file):
    document_text = load_pdf(pdf_file.name)

    # Split text into smaller chunks for faster processing
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
    docs = text_splitter.create_documents([document_text])

    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-MiniLM-L6-v2")
    vector_store = Chroma.from_documents(docs, embedding=embedding_model, persist_directory="./chroma_db")
    llm = CTransformers(model="TheBloke/Mistral-7B-Instruct-v0.1-GGUF", model_type="mistral", device="cuda")

    #llm = CTransformers( model="distilgpt2", model_type="gpt2", device="cuda")
    retriever = vector_store.as_retriever()
    qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever, chain_type="stuff")

    # Generate the summary
    summary = qa_chain.run("Summarize the document in simple terms.")
    print(summary)

    return summary

# Gradio Interface
iface = gr.Interface(fn=summarize_pdf,
                     inputs=gr.File(label="Upload PDF"),
                     outputs=gr.Textbox(label="Summary"))

# Launch with share=True
iface.launch(share=True)